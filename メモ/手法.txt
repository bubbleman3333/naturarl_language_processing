ベクトル化のアプローチは主に二つ
１つはカウントベース、もう一つは推論ベース

これらはともに分布仮説がもとになっている


カウントベースでは、コーパス全体で学習が必要になる、
推論ベースではミニバッチ学習をして逐次的に学習を行うことが出来る


推論ベースの概要

You [?] goodbye and I say hello.

この文章が来た時に、?の中にどのような単語が出現するのかを推論する

最初のステップとして、単語をone-hotで表現する

["a","b"]というように単語ごとにインデックスを割り振っておき、aがきたら
[1,0]というように1のフラグだけ立てておく
コンテクストを2とすれば、入力層は2、つまりone-hotが2つになるようにする

中間層のニューロンの変換後は、h1とh2の平均をとる

o
o
o
o
o
o
        ⇒⇒⇒ o   ←ここで平均をとるイメージ
              o
              o
o
o
o
o
o

スコアにはsoftmaxを適用して、確率を得る。


point⇒中間層のニューロン数を入力層のニューロン数よりも少なくする
こうすることによって、単語を予測することに必要な情報をコンパクトにまとめる

ニューラルネットワークによって学習をすると、コンテキストをうまく学習できるようになっている

ワンホットベクトルでは該当の単語に該当するインデックス以外全てが0
word2vecの改良⇒embeddingレイヤを導入して必要な単語のベクトルの実を抽出するように改修、こうすることによって計算時間を大幅に短縮できる

問題は中間層の処理、全ての単語に対してスコアを計算する必要があるため、計算量はやはりボトルネックになる
また、ソフトマックス関数を使うのも、語彙数が100万あった場合にとても時間がかかる

⇒改良案、negative sampling


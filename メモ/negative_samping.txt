negative samplingのポイント⇒
多値分類を二値分類に近似する

例：コンテキストがyouとgoodbyeのとき、間に入る単語は何ですか？←これが従来
コンテキストがyouとgoodbyeのとき、間に入る単語はsayですか？という質問に近似する

この場合、sayに対応するベクトルだけ作ればいいので出力層のニューロンの数を1個にできる

スコアにシグモイド関数を適用することで、結果を0～1に確率を得ることが出来る。
結果に関しては、交差エントロピー誤差を利用する

シグモイド関数に対して使用される損失関数、交差エントロピー誤差は以下の式

L = - (t*logy + (1-t)*log(1-y))


これをすることによって正解データに対してのみ、学習をすることが出来る。
ただし、学んだのは正の値の身なので、不正解のデータに対しては学ぶことが出来ない。つまり、0に近づけることが出来ない

これを避けるのがネガティブサンプリングという手法
不正解な例をいくつかサンプリングして0に近づける学習をさせるという手法


ネガティブサンプリングの抽出方法については、よく使われる単語を抽出するようにする。
コーパスの単語の出現数をまとめ、これを確立分布とする
高頻度な単語に対応できた方が、いい結果につながる


numpyの確率分布に従ってサンプリング

p = [0.5,0.2,0.3]
s = np.random.choice(words,p=p)
上記の例に従い、データがサンプリングされるようになる

確率分布を確率分布のまま扱わず、0.75乗することが推奨されている。
これは、出現頻度が低い単語を見捨てさせないため⇒少しだけサンプリングさせやすくする